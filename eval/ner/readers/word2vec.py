import numpy as np

from readers.vocab import read_vocab
from sequences.label_dictionary import LabelDictionary
from util.util import line_reader, nparr_to_str


"""
Prepare a word to embedding map.

embedding file:
26829 128
</s> 0.003127 0.003453 -0.002992 -0.002561 0.001068 0.002360 0.000735 0.000165 -0.002815 0.001733 -0.003403 0.000976 -0.000587 -0.000748 -0.002590 -0.001470 0.002015 0.002364 0.002320 0.001248 0.001207 -0.002971 -0.003200 0.003883 0.002970 0.002414 -0.000472 0.003137 -0.000387 0.000574 -0.000117 -0.002331 0.001025 -0.001045 -0.002989 0.003714 0.003421 -0.000856 -0.000177 0.000398 -0.002842 -0.003131 0.003558 0.000049 -0.002017 -0.002377 -0.002403 0.001326 0.000157 0.001040 -0.003292 -0.002975 -0.000102 0.000893 0.001992 -0.002477 0.003187 0.000724 0.000875 -0.000475 0.002333 -0.001787 -0.001639 0.001686 -0.000588 0.000805 0.001410 -0.003194 -0.001549 0.002276 0.003306 0.003072 -0.002380 -0.001647 -0.000710 0.001564 -0.002960 0.002342 0.002178 -0.001249 -0.001212 -0.001749 0.003304 0.003056 -0.000922 0.003293 0.003766 0.001418 0.003893 -0.002431 -0.001197 -0.001646 -0.002271 0.002199 0.001234 0.000332 -0.001714 0.001191 0.002390 0.000151 0.001781 -0.003869 0.003371 0.000375 -0.001431 -0.001628 -0.000584 -0.002293 -0.003414 -0.002216 0.003341 -0.001383 -0.002093 0.001810 -0.001331 0.003716 -0.001254 -0.003585 0.002122 -0.000058 -0.002726 0.000762 0.001127 -0.000376 0.001425 -0.003092 0.000688 0.001419
, 0.222430 -0.033822 0.178735 -0.138123 0.093112 0.019595 0.179342 -0.009328 -0.196602 0.002852 0.190311 0.006021 0.048953 -0.104371 -0.024252 -0.114658 -0.037334 -0.006529 -0.070585 -0.051363 0.025941 0.229906 0.131894 -0.044501 0.072037 0.113268 -0.176375 0.222747 -0.007233 -0.066835 0.060084 -0.002108 -0.025276 -0.038596 0.007100 0.083996 -0.054703 -0.138920 -0.164588 -0.066337 0.180729 -0.113548 -0.061572 0.029827 -0.011307 0.150372 0.001863 0.130876 0.017121 -0.181130 0.200711 -0.281501 0.333229 0.058091 -0.120617 0.088288 0.048136 -0.017782 -0.005923 -0.229044 0.201729 0.012985 0.141589 0.195968 -0.050028 0.013533 0.077680 0.110517 0.186077 -0.082399 0.147388 -0.023397 0.170147 -0.094358 -0.007849 -0.197371 0.047478 -0.085036 -0.015667 0.086090 0.081577 0.122968 -0.385172 -0.103821 -0.044591 -0.011249 -0.108602 -0.002437 0.183937 0.021459 -0.116275 0.119091 -0.123960 -0.193334 -0.153501 -0.085771 0.025466 0.141956 0.028236 -0.214562 0.018657 0.235615 0.196678 -0.304848 -0.173338 -0.171926 -0.210296 -0.018270 0.144885 0.113631 -0.108080 0.100092 0.105808 -0.290642 -0.039535 -0.079398 0.111163 -0.209248 0.027580 0.105613 0.184000 0.108205 0.194882 0.016112 -0.017421 0.015373 -0.114119 -0.008947
. 0.224195 0.085053 0.167609 -0.022000 -0.112262 -0.012534 0.168536 -0.158865 -0.314433 0.014393 -0.020631 0.214419 -0.034719 0.088953 0.068915 -0.033907 -0.059711 0.087487 -0.044388 -0.030766 0.162496 0.174703 0.152376 -0.233632 0.176610 0.067744 -0.200449 -0.146416 0.062976 -0.069436 -0.089321 0.153515 0.101748 -0.064910 -0.132097 -0.123970 0.200603 -0.245511 -0.170530 -0.164728 0.005117 0.058490 0.142940 0.124228 -0.127804 0.102198 -0.065731 0.245338 0.070209 -0.055180 0.190213 -0.137834 0.040475 0.182198 -0.233516 0.037950 -0.070744 -0.041022 0.183150 0.053304 -0.022883 0.097468 0.350665 0.143042 -0.120487 -0.025186 0.285722 -0.245808 0.176918 -0.014552 -0.087902 -0.189719 0.032869 0.042718 -0.064986 -0.118345 0.130195 -0.054462 -0.121747 0.210747 -0.050143 0.160825 -0.455471 -0.108482 -0.105540 0.018879 0.055249 0.137885 0.329582 0.086169 -0.067811 0.141499 -0.191367 -0.227707 0.009890 -0.046475 -0.059299 0.005324 0.006412 -0.138107 0.206108 0.247331 0.145805 -0.226303 -0.009878 -0.128703 0.051967 -0.035339 0.067358 0.159157 -0.096253 0.287785 -0.029869 -0.252310 0.044813 -0.127213 0.090435 -0.174669 -0.104594 0.032717 -0.069939 0.024389 0.213188 0.097629 -0.008749 0.046151 -0.273576 0.032848

"""


def load_embed(embed_f, vocab_f):
    """
    Reads the embedding file and returns the numpy matrix, where row ids corresponds to vocab ids
    """
    w_dict = LabelDictionary(read_vocab(vocab_f))
    with open(embed_f) as in_f:
        m, n = map(eval, in_f.readline().strip().split())
    e_m = np.zeros((m - 1, n))  # embedding matrix; m-1 to leave out </s>
    for l in line_reader(embed_f, skip=1):
        w, *e = l.strip().split()
        assert len(e) == n

        if w not in w_dict:
            continue
        e_m[w_dict.get_label_id(w)] = e
    return e_m


def read_embed(embed_f):
    """
    simply read the embedding file into two structures, one for vocabulary and one for embeddings
    """
    w_to_emb = {}
    for c, l in enumerate(line_reader(embed_f)):
        if c == 0:
            m, n = map(eval, l.strip().split())
            continue
        w, *e = l.strip().split()
        assert len(e) == n
        w_to_emb[w] = e

    return w_to_emb


def write_embed(data, f, header):
    """
    write to file in embedding word2vec format
    """

    def format(word, nparray):
        return "{} {}\n".format(word, nparr_to_str(nparray))

    with open(f, "w") as out_f:
        out_f.write(header)
        for word, nparray in data:
            out_f.write(format(word, nparray))


def write_embed_semafor(data, embed_f, vocab_f):
    """
    in semafor format, i.e. separate vocabulary and embedding files
    :param data: w_to_emb dictionary
    """
    with open(embed_f, "w") as embed_out, open(vocab_f, "w") as vocab_out:
        for w, e in data.items():
            embed_out.write("{}\n".format(" ".join(e)))
            vocab_out.write(w + "\n")




